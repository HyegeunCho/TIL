{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그리드 세계 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "\n",
    "class gameEnv():\n",
    "    def __init__(self, partial, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(), 1, 1, 2, None, 'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole)\n",
    "        bug2 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole2)\n",
    "        bug3 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug3)\n",
    "        bug4 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    def moveChar(self, direction):\n",
    "        # 0-up, 1-down, 2-left, 3-right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.\n",
    "        \n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY - 2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX - 2:\n",
    "            hero.x += 1\n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 0.0\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "        \n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        \n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 1, 1, 'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 0, -1, 'fire'))\n",
    "                return other.reward, False\n",
    "        if ended == False:\n",
    "            return 0.0, False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        # a = np.zeros([self.sizeY, self.sizeX, 3])\n",
    "        a = np.ones([self.sizeY + 2, self.sizeX + 2, 3])\n",
    "        a[1:-1, 1:-1, :] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            a[item.y + 1 : item.y + item.size + 1, item.x + 1 : item.x + item.size + 1, item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            a = a[hero.y:hero.y3, hero.x:hero.x+3, :]\n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b, c, d], axis=2)\n",
    "        return a\n",
    "    def step(self, action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward, done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        if reward == None:\n",
    "            print(done)\n",
    "            print(reward)\n",
    "            print(penalty)\n",
    "            return state, (reward+penalty), done\n",
    "        else:\n",
    "            return state, (reward+penalty), done\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게임 환경 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122114630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        # 네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다.(flattening)\n",
    "        # 그 다음 배열의 크기를 재조절하고 4개의 합성곱 계층을 거쳐 처리한다.\n",
    "        self.scalarInput = tf.placeholder(shape=[None,21168], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.conv2d(inputs=self.imageIn, num_outputs=32, kernel_size=[8, 8], stride=[4,4], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d(inputs=self.conv1, num_outputs=64, kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d(inputs=self.conv2, num_outputs=64, kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d(inputs=self.conv3, num_outputs=h_size, kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        # 마지막 합성곱 계층에서 출력값을 취한 후\n",
    "        # 이를 어드밴티지 스트림과 가치 스트림으로 분리한다.\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size // 2, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size // 2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # 최종 Q값을 얻기 위해 어드밴티지 스트림과 가치 스트림을 조합한다.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # 타깃 Q값과 예측 Q값의 차의 제곱합을 구함으로써 비용을 얻는다.\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경험 리플레이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])\n",
    "\n",
    "# 게임의 프레임의 크기를 재조절해주는 함수\n",
    "def processState(states):\n",
    "    return np.reshape(states, [21168])\n",
    "\n",
    "# 1차 네트워크의 매개변수에 맞춰 타깃 네트워크의 매개변수를 업데이트\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tfVars[0:total_vars // 2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value() * tau) + ((1 - tau) * tfVars[idx + total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 학습\n",
    "\n",
    "학습 매개변수를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # 각 학습 단계에서 사용할 경험 배치의 수\n",
    "update_freq = 4 # 학습 단계 업데이트 빈도\n",
    "y = .99 # 타깃 Q값에 대한 할인 계수\n",
    "startE = 1 # 랜덤한 액션을 시작할 가능성\n",
    "endE = 0.1 # 랜던한 액션을 끝낼 가능성\n",
    "anneling_steps = 10000 # startE에서 endE로 줄어드는 데 필요한 학습 단계 수\n",
    "num_episodes = 10000 # 네트워크를 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 10000 # 학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 # 허용되는 최대 에피소드 길이\n",
    "load_model = False # 저장된 모델을 로드할 지 여부\n",
    "path = './dqn' # 모델을 저장할 경로\n",
    "h_size = 512 # 어드밴티디 / 가치 스트림으로 분리되기 전 마지막 합성곱 계층의 크기\n",
    "tau = 0.001 # 타깃 네트워크를 제 1 네트워크로 업데이트 하는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ef684fc22ef8>:23: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 1.9 1\n",
      "1000 3.7 1\n",
      "1500 3.0 1\n",
      "2000 2.9 1\n",
      "2500 2.9 1\n",
      "3000 2.7 1\n",
      "3500 1.9 1\n",
      "4000 2.5 1\n",
      "4500 1.1 1\n",
      "5000 1.8 1\n",
      "5500 1.8 1\n",
      "6000 1.6 1\n",
      "6500 2.7 1\n",
      "7000 1.8 1\n",
      "7500 0.5 1\n",
      "8000 2.8 1\n",
      "8500 1.1 1\n",
      "9000 2.9 1\n",
      "9500 1.6 1\n",
      "10000 1.7 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mainQn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8236a670b452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdoubleQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mend_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# 타깃 값을 이용해 네트워크를 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;31m# 타깃 네트워크가 제1네트워크와 동일하도록 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mainQn' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# 랜덤 액션이 감소하는 비율을 설정\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# 보상의 총계와 에피소드별 단계 수를 담을 리스트를 생성\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# 모델을 저장할 경로를 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    # 타깃 네트우크가 제 1네트워크와 동일하도록 설정\n",
    "    updateTarget(targetOps, sess)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        # 환경을 리셋하고 첫번째 새로운 관찰을 얻는다.\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        # Q 네트워크\n",
    "        # 에이전트가 블록에 도달하기까지 최대 50회 시도하고 종료\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # Q 네트워크에서 (e의 확률로 랜덤한 액션과 함께) 그리디하게 액션을 선택\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            # 에피소드 버퍼에 경험을 저장\n",
    "            episodeBuffer.add(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 경험에서 랜덤하게 배치 하나를 샘플링\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    # 타깃 Q값에 대해 더블 DQN 업데이트를 수행\n",
    "                    Q1 = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout, feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size), Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y * doubleQ * end_multiplier)\n",
    "                    # 타깃 값을 이용해 네트워크를 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    # 타깃 네트워크가 제1네트워크와 동일하도록 설정\n",
    "                    updateTarget(targetOps, sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "        \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # 정기적으로 모델 저장\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "            print('Saved Model')\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps, np.mean(rList[-10:]), e)\n",
    "    saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of successful episodes: \" + str(sum(rList) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(rList), [len(rList)//100, 100])\n",
    "rMean = np.avarage(rMat, 1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
