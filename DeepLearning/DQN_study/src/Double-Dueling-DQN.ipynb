{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그리드 세계 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "\n",
    "class gameEnv():\n",
    "    def __init__(self, partial, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(), 1, 1, 2, None, 'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole)\n",
    "        bug2 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole2)\n",
    "        bug3 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug3)\n",
    "        bug4 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(bug4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    def moveChar(self, direction):\n",
    "        # 0-up, 1-down, 2-left, 3-right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.\n",
    "        \n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY - 2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX - 2:\n",
    "            hero.x += 1\n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 0.0\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "        \n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        \n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 1, 1, 'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 0, -1, 'fire'))\n",
    "                return other.reward, False\n",
    "        if ended == False:\n",
    "            return 0.0, False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        # a = np.zeros([self.sizeY, self.sizeX, 3])\n",
    "        a = np.ones([self.sizeY + 2, self.sizeX + 2, 3])\n",
    "        a[1:-1, 1:-1, :] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            a[item.y + 1 : item.y + item.size + 1, item.x + 1 : item.x + item.size + 1, item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            a = a[hero.y:hero.y3, hero.x:hero.x+3, :]\n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b, c, d], axis=2)\n",
    "        return a\n",
    "    def step(self, action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward, done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        if reward == None:\n",
    "            print(done)\n",
    "            print(reward)\n",
    "            print(penalty)\n",
    "            return state, (reward+penalty), done\n",
    "        else:\n",
    "            return state, (reward+penalty), done\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게임 환경 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12225d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        # 네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다.(flattening)\n",
    "        # 그 다음 배열의 크기를 재조절하고 4개의 합성곱 계층을 거쳐 처리한다.\n",
    "        self.scalarInput = tf.placeholder(shape=[None,21168], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.conv2d(inputs=self.imageIn, num_outputs=32, kernel_size=[8, 8], stride=[4,4], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d(inputs=self.conv1, num_outputs=64, kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d(inputs=self.conv2, num_outputs=64, kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d(inputs=self.conv3, num_outputs=h_size, kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        # 마지막 합성곱 계층에서 출력값을 취한 후\n",
    "        # 이를 어드밴티지 스트림과 가치 스트림으로 분리한다.\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size // 2, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size // 2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # 최종 Q값을 얻기 위해 어드밴티지 스트림과 가치 스트림을 조합한다.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # 타깃 Q값과 예측 Q값의 차의 제곱합을 구함으로써 비용을 얻는다.\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경험 리플레이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])\n",
    "\n",
    "# 게임의 프레임의 크기를 재조절해주는 함수\n",
    "def processState(states):\n",
    "    return np.reshape(states, [21168])\n",
    "\n",
    "# 1차 네트워크의 매개변수에 맞춰 타깃 네트워크의 매개변수를 업데이트\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tfVars[0:total_vars // 2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value() * tau) + ((1 - tau) * tfVars[idx + total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 학습\n",
    "\n",
    "학습 매개변수를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # 각 학습 단계에서 사용할 경험 배치의 수\n",
    "update_freq = 4 # 학습 단계 업데이트 빈도\n",
    "y = .99 # 타깃 Q값에 대한 할인 계수\n",
    "startE = 1 # 랜덤한 액션을 시작할 가능성\n",
    "endE = 0.1 # 랜던한 액션을 끝낼 가능성\n",
    "anneling_steps = 10000 # startE에서 endE로 줄어드는 데 필요한 학습 단계 수\n",
    "num_episodes = 10000 # 네트워크를 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 10000 # 학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 # 허용되는 최대 에피소드 길이\n",
    "load_model = False # 저장된 모델을 로드할 지 여부\n",
    "path = './dqn' # 모델을 저장할 경로\n",
    "h_size = 512 # 어드밴티디 / 가치 스트림으로 분리되기 전 마지막 합성곱 계층의 크기\n",
    "tau = 0.001 # 타깃 네트워크를 제 1 네트워크로 업데이트 하는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ef684fc22ef8>:23: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 2.1 1\n",
      "1000 2.2 1\n",
      "1500 1.3 1\n",
      "2000 3.1 1\n",
      "2500 2.4 1\n",
      "3000 1.4 1\n",
      "3500 3.0 1\n",
      "4000 1.4 1\n",
      "4500 1.6 1\n",
      "5000 2.5 1\n",
      "5500 2.2 1\n",
      "6000 2.8 1\n",
      "6500 1.2 1\n",
      "7000 2.7 1\n",
      "7500 1.3 1\n",
      "8000 2.5 1\n",
      "8500 1.6 1\n",
      "9000 2.8 1\n",
      "9500 2.5 1\n",
      "10000 1.8 1\n",
      "10500 1.2 0.9549999999999828\n",
      "11000 2.4 0.9099999999999655\n",
      "11500 1.3 0.8649999999999483\n",
      "12000 1.3 0.819999999999931\n",
      "12500 2.6 0.7749999999999138\n",
      "13000 3.0 0.7299999999998965\n",
      "13500 3.0 0.6849999999998793\n",
      "14000 2.2 0.639999999999862\n",
      "14500 2.1 0.5949999999998448\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# 랜덤 액션이 감소하는 비율을 설정\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# 보상의 총계와 에피소드별 단계 수를 담을 리스트를 생성\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# 모델을 저장할 경로를 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    # 타깃 네트우크가 제 1네트워크와 동일하도록 설정\n",
    "    updateTarget(targetOps, sess)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        # 환경을 리셋하고 첫번째 새로운 관찰을 얻는다.\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        # Q 네트워크\n",
    "        # 에이전트가 블록에 도달하기까지 최대 50회 시도하고 종료\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # Q 네트워크에서 (e의 확률로 랜덤한 액션과 함께) 그리디하게 액션을 선택\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            # 에피소드 버퍼에 경험을 저장\n",
    "            episodeBuffer.add(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 경험에서 랜덤하게 배치 하나를 샘플링\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    # 타깃 Q값에 대해 더블 DQN 업데이트를 수행\n",
    "                    Q1 = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout, feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size), Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y * doubleQ * end_multiplier)\n",
    "                    # 타깃 값을 이용해 네트워크를 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    # 타깃 네트워크가 제1네트워크와 동일하도록 설정\n",
    "                    updateTarget(targetOps, sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "        \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # 정기적으로 모델 저장\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "            print('Saved Model')\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps, np.mean(rList[-10:]), e)\n",
    "    saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of successful episodes: \" + str(sum(rList) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(rList), [len(rList)//100, 100])\n",
    "rMean = np.avarage(rMat, 1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
