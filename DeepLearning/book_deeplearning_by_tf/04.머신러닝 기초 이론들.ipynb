{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Batch Gradient Descent, Mini-Batch Gradient Descent, Stochastic Gradient Descent\n",
    "\n",
    "머신러닝 모델의 기본 프로세스\n",
    "\n",
    "1. 학습하고자 하는 가설 h($\\theta$) 을 수학적 표현식으로 나타낸다.\n",
    "2. 가설의 성능을 측정할 수 있는 손실함수 J($\\theta$)을 정의한다.\n",
    "3. 손실함수 J($\\theta$)을 최소화할 수 있는 학습 알고리즘을 설계한다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3번의 손실함수를 최소화할 수 있는 최적화 알고리즘으로 가장 일반적으로 사용되는 기법이 경사하강법\n",
    "손실함수의 미분값과 러닝레이트의 곱만큼을 원래 파라미터에서 뺀 값으로 파라미터를 업데이트\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_i = \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J\\left( \\theta_0, \\theta_1 \\right)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 트레이닝 데이터가 1000(n)개 라면 이를 100(m)개씩 묶은 Mini-Batch 개수만큼의 손실 함수 미분값 평균을 이용해서 파라미터를 한 스텝씩 업데이트하는 기법\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {\\partial} {\\partial \\theta_0} J \\left( \\theta_0, \\theta_1 \\right) = \\frac {\\partial} {\\partial \\theta_0} \\frac {1} {2 m} \\sum_{i=1}^m \\left(\\overset \\wedge {y_l} - y_i \\right) ^ 2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
